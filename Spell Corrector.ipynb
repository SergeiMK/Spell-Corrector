{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell correction system implementation (NLP course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "## 1. Introduction\n",
    "### 1.1 Task\n",
    "### 1.2 Our approach\n",
    "### 1.3 Corpus used\n",
    "### 1.4 Generating and ranking candidates \n",
    "### 1.5 Perfomance evaluation\n",
    "### 1.6 Wrap-up\n",
    "## 2. Creating the dictionary\n",
    "## 3. Norviq spell corrector (baseline model)\n",
    "## 4. Phonetics matching algorithms\n",
    "### 4.1 Soundex\n",
    "### 4.2 Metaphone\n",
    "### 4.3 MRA (match rating approach)\n",
    "## 5 Word distance matching algorithms\n",
    "### 5.1 Levenstein distance\n",
    "### 5.2 Damerau-Levenstien distance\n",
    "### 5.3 Jaro-Whinkler distance\n",
    "### 5.4 Hamming distance\n",
    "### 5.5 Levenstein and Damerau-Levenstien weighted based on Keyboard Physical Distance (QWERTY)\n",
    "## 6 Ngrams (based on characters in the words)\n",
    "### 5.1 Unigrams\n",
    "### 5.2 Bigrams\n",
    "## 7 Models comparison\n",
    "## 8 User interface for system testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a spell correction system (it's quite open ended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checking is an essential part of many applications. However, its working characteristics (speed, quality, memory consumption) are often not optimal.\n",
    "\n",
    "10+ years ago Peter Norvig has put his sterlingly simple spell-checker into 21 line of python code. \n",
    "\n",
    "https://norvig.com/spell-correct.html\n",
    "\n",
    "A pure probability model selects the most probable replacement for an unknown word from the list of words collected in a book – this minimalistic approach is still quite relevant and can be considered as a baseline and a prototype for further development. \n",
    "\n",
    "However, it has a number of flaws:\n",
    "\n",
    " - If our dictionary will be essentially big (for instance, like millions of words), the candidate list will be too big to rank it properly with this simple technique\n",
    " - No restrictions on the speed and memory are provided for this solution\n",
    " - It cannot find real-word errors\n",
    "\n",
    "As the starting point (baseline model) we will take Peter Norvig spell corrector and try to compare/beat it with the algorithms we have learned during the NLP course:\n",
    " - Phonetics group \n",
    " - Word distance matching techniques \n",
    " - Ngrams (based on characters in the words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using slightly pre-processed 'big.txt' file, that can be obtained from Peter Norvig’s web site.\n",
    "\n",
    "https://www.norvig.com/ngrams/\n",
    "\n",
    "The corpus contains a concatenation of several public domain books from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus. We can assume that the words in the 'big.txt' file are spelled correctly. \n",
    "\n",
    "In order to validate our model we will use the a list of 384 misspelled words (test-words-misspelled.txt) and their corresponding correct spellings (test-words-correct.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and ranking candidates \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each algorithm the generation of candidates was implemented differently:\n",
    "\n",
    " - By choosing the strings' minimum betweeness distance (if the algorithm is measuring the actual distance between two strings: Phonetics and most of Edit Distance models)\n",
    " - By choosing the words with maximum index distance (if the algorithm is measuring the distance in the index format: from 0 to 1, Jaro-Winkler and Levenstien Wieghted Keyboard distance)\n",
    " - Ngrams (by n-characters frequency distribution comparison)\n",
    "\n",
    " \n",
    "Ranking of final pool of candidates was implemented in accordance with probablity of candidate occurence in a given corpus (as done for Norviq's version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have evaluted algorithms based on two performance aspects:\n",
    "\n",
    " - Accuracy: Algorithms predict only one most possible candidate (headword). When it is exact the correct headword, we count that as a correct prediction. Then calculate the proportion of the number of being successfully predicted to the number of attempted headwords. High accuracy means the method can output a single word and the word is the right prediction.\n",
    "\n",
    " - Time cost: Time will be recorded from the start of the program to the end. And this includes the time spent on calculating accuracy.\n",
    "\n",
    "\n",
    "For the future development it is recommended to take into account the following two metrics as well:\n",
    "\n",
    " - Precision (Algorithms predict one or more possible candidates. Count as a correct prediction when the correct candidate is in   the result. Calculate the proportion of the number of being successfully predicted to the number of whole prediction result). High precision means the method can output the right word out of not too many predictions\n",
    "\n",
    " - Recall ratio (Algorithms predict one or more possible candidates. Based on that we calculate the proportion of the number of being successfully predicted to the number of attempted variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results\n",
    "The results showed the best perfomance for Damerau-Levenstein approach, which has slightly outperfomed Norviq's accuracy (73.96 % vs. 73.7 %). But underscores on time speed (29.3 vs 8.4 seconds for correction)\n",
    "\n",
    "\n",
    "### Possible Additional Improvements\n",
    "\n",
    "During the system testing we have explored that searching of a word in a plain dictionary or list structure can take a while (especially for some computational consuming algorithms), and it can take even more if you should measure a distance between an out-of-vocabulary word and every word in a dictionary to find corrections.\n",
    "\n",
    "That's why for the further time cost optimizations it will be advantageous to switch to an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. It is kind of similar to a binary search tree, but for language data - you have no > < conditions\n",
    "\n",
    "https://en.wikipedia.org/wiki/Trie\n",
    "\n",
    "The other option could be the usage of bk-trees. Instead of plain dictionaries is that measuring the distance between an out-of-vocabulary word and every word in the dictionary is much faster in this kind of structure - it’s now O(log n) instead of O(n).\n",
    "\n",
    "https://en.wikipedia.org/wiki/BK-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "from collections import Counter\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the text\n",
    "text = open('C:/Users/serge/Desktop/Datasets/big.txt', 'r', encoding='utf-8').read()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove numbers\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuations and convert characters to lower case\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
    "    \n",
    "    # Substitute multiple whitespace with single whitespace # Also, removes leading and trailing whitespaces\n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    return text_no_doublespace\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "\n",
    "def tokenize (raw):\n",
    "   \n",
    "    #Tokenizening\n",
    "    words = word_tokenize(raw)\n",
    "    \n",
    "    #Filtering the words (since we have a test sample with words length>2)\n",
    "    words=[words for words in words if len(words) > 2]\n",
    "\n",
    "    # Deleting stop_words\n",
    "    stop_words = stopwords.words('English')\n",
    "    words = [i for i in words if (i not in stop_words)]\n",
    "    \n",
    "\n",
    "    return words\n",
    "\n",
    "vocab=tokenize (cleaned_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547217"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of words in dict:\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35168"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of unique words\n",
    "vocabulary=set(vocab)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expulsion',\n",
       " 'packed',\n",
       " 'berkshire',\n",
       " 'filter',\n",
       " 'choirs',\n",
       " 'spongiopilene',\n",
       " 'combinations',\n",
       " 'carefree',\n",
       " 'threaten',\n",
       " 'gibe',\n",
       " 'hydraulics',\n",
       " 'diaz',\n",
       " 'powder',\n",
       " 'hobart',\n",
       " 'dont',\n",
       " 'fragments',\n",
       " 'byproducts',\n",
       " 'classes',\n",
       " 'bosnia',\n",
       " 'phleboliths',\n",
       " 'faculty',\n",
       " 'immensity',\n",
       " 'prepatellar',\n",
       " 'reflections',\n",
       " 'conversion',\n",
       " 'unsolicited',\n",
       " 'exasperated',\n",
       " 'peasantsstreamed',\n",
       " 'danielll',\n",
       " 'explanatory']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary=list(vocabulary)\n",
    "vocabulary[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norvig Spell corrector as the starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norvig spell corrector finds out all the words that ate one or two or multiple edit distances away from the given word. It finally assigns correct spelling to the word which has the maximum probability of occurence in a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(vocab)\n",
    "\n",
    "wordcount = []\n",
    "for value in WORDS.values():\n",
    "    wordcount.append(value)\n",
    "\n",
    "total = sum(wordcount)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def P(word): \n",
    "    N = total\n",
    "    \"Probability of `word`.\"\n",
    "    return (WORDS[word] / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 3456),\n",
       " ('one', 3215),\n",
       " ('may', 2538),\n",
       " ('would', 1949),\n",
       " ('prince', 1893),\n",
       " ('pierre', 1785),\n",
       " ('could', 1695),\n",
       " ('time', 1509),\n",
       " ('man', 1502),\n",
       " ('new', 1200),\n",
       " ('first', 1155),\n",
       " ('well', 1143),\n",
       " ('old', 1138),\n",
       " ('face', 1122),\n",
       " ('upon', 1108),\n",
       " ('men', 1104),\n",
       " ('see', 1094),\n",
       " ('natasha', 1093),\n",
       " ('two', 1071),\n",
       " ('andrew', 1065),\n",
       " ('french', 1059),\n",
       " ('know', 1041),\n",
       " ('like', 1017),\n",
       " ('without', 1006),\n",
       " ('went', 1005),\n",
       " ('made', 999),\n",
       " ('little', 997),\n",
       " ('came', 976),\n",
       " ('states', 963),\n",
       " ('must', 954)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just some EDA to explore the file we use as bag of words\n",
    "WORDS.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'said'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most popular word\n",
    "max(WORDS, key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'areas', 'tread', 'treat', 'treats', 'trees', 'tres'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the candidates for correction\n",
    "candidates('treas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trees'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the actual correction\n",
    "correction('treas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cand=list(candidates('treas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tread 2.1929143283194784e-05\n",
      "tres 7.309714427731595e-06\n",
      "treat 5.665028681491986e-05\n",
      "areas 7.492457288424885e-05\n",
      "trees 9.319885895357783e-05\n",
      "treats 3.6548572138657973e-06\n"
     ]
    }
   ],
   "source": [
    "#We can see that it ranks candidates according to probability in the dictionary\n",
    "for i in list_cand:\n",
    "    print (i, P(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonetic Matching Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norvig Spell corrector is using standard distance measure, which is based on letter alignment.\n",
    "That's why this search is limited to candidates standing 1-2 letters from a word with an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'treeless'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test new things\n",
    "correction('treeeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'riiiiight'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('riiiiight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('riiight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that Norvig Spell corrector can't handle such type of mistakes, because it is a typical errors, caused by intentional distortion or slangy language gamification, outstand from a relevant candidate more than 2 letters away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test how Phonetics algorithms will deal with this we will use pyphonetics library and test three models: Soundex (its refined version), Metaphone and Match rating approach. In order to calculate rank candidates we will use method 'distance'  to find the distance between 2 phonetic representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyphonetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphonetics import Soundex\n",
    "from pyphonetics import RefinedSoundex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling.The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Method was originally developed by Margaret Odell and Robert Russell. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T6903'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundex = RefinedSoundex()\n",
    "soundex.phonetics('treeeeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T6903'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundex.phonetics('trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundex.distance('treeeeees', 'trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_soundex(word):\n",
    "    soundex = RefinedSoundex()\n",
    "    cand_list =[]\n",
    "    best_soundex= 0\n",
    "    for current_string in vocabulary:\n",
    "        current_score = soundex.distance(word, current_string)\n",
    "        if(current_score == best_soundex):\n",
    "            best_soundex = current_score\n",
    "            cand_list.append(current_string)\n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['truck',\n",
       " 'truss',\n",
       " 'tres',\n",
       " 'threes',\n",
       " 'trash',\n",
       " 'tricks',\n",
       " 'thresh',\n",
       " 'trees',\n",
       " 'trick',\n",
       " 'throws',\n",
       " 'tracks',\n",
       " 'tries',\n",
       " 'track']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_soundex('treeeeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try more complex one with riiiiight\n",
    "candidates_soundex('riiiiight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_soundex_bestprobability(word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    try:\n",
    "        return max(candidates_soundex (word), key=P)\n",
    "    except:\n",
    "        ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trees'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_soundex_bestprobability ('treeeeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_soundex_bestprobability('riiiiight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of correction are impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metaphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphonetics import Metaphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar-sounding words should share the same keys.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Metaphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more simple words Metaphone makes a phonetic hash out of each word in the vocabulary and getting this kind of a hash for a word with error, you can search for a better candidate through your hashed dictionary using standard distance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRS'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphone=Metaphone()\n",
    "metaphone.phonetics('treeeeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRS'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphone.phonetics('trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphone = Metaphone()\n",
    "metaphone.distance('treeeeees', 'trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_metaphone(word):\n",
    "    metaphone = Metaphone()\n",
    "    cand_list =[]\n",
    "    best_metaphone= 0\n",
    "    for current_string in vocabulary:\n",
    "        current_score = metaphone.distance(word, current_string)\n",
    "        if(current_score == best_metaphone):\n",
    "            best_metaphone = current_score\n",
    "            cand_list.append(current_string)\n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trace',\n",
       " 'drowsy',\n",
       " 'troughs',\n",
       " 'drissa',\n",
       " 'diaries',\n",
       " 'truce',\n",
       " 'dorsi',\n",
       " 'truss',\n",
       " 'tres',\n",
       " 'trousseau',\n",
       " 'doers',\n",
       " 'terse',\n",
       " 'dorrs',\n",
       " 'daresay',\n",
       " 'draws',\n",
       " 'dares',\n",
       " 'dries',\n",
       " 'tierce',\n",
       " 'trees',\n",
       " 'dress',\n",
       " 'tiers',\n",
       " 'doors',\n",
       " 'tories',\n",
       " 'tears',\n",
       " 'tries',\n",
       " 'taras',\n",
       " 'terrace',\n",
       " 'darcy',\n",
       " 'tires',\n",
       " 'teres']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_metaphone('treeeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ride',\n",
       " 'riot',\n",
       " 'ready',\n",
       " 'road',\n",
       " 'reid',\n",
       " 'wrought',\n",
       " 'rid',\n",
       " 'read',\n",
       " 'rude',\n",
       " 'rod',\n",
       " 'ruddy',\n",
       " 'root',\n",
       " 'reed',\n",
       " 'raid',\n",
       " 'rowdy',\n",
       " 'rat',\n",
       " 'rout',\n",
       " 'red',\n",
       " 'rite',\n",
       " 'write',\n",
       " 'wright',\n",
       " 'rode',\n",
       " 'rut',\n",
       " 'wrote',\n",
       " 'writ',\n",
       " 'right',\n",
       " 'rot',\n",
       " 'route',\n",
       " 'rate',\n",
       " 'rete',\n",
       " 'radio']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try more complex one with riiiiight\n",
    "candidates_metaphone('riiiiight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_metaphone_bestprobability(word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    try:\n",
    "        return max(candidates_metaphone (word), key=P)\n",
    "    except:\n",
    "        ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tears'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_metaphone_bestprobability ('treeeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_metaphone_bestprobability ('riiiiight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Rating Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match rating approach (MRA) is a phonetic algorithm developed by Western Airlines in 1977 for the indexation and comparison of homophonous names.\n",
    "\n",
    "The algorithm itself has a simple set of encoding rules but a more lengthy set of comparison rules. The main mechanism is the similarity comparison, which calculates the number of unmatched characters by comparing the strings from left to right and then from right to left, and removing identical characters. This value is subtracted from 6 and then compared to a minimum threshold. The minimum threshold is defined in table A and is dependent upon the length of the strings.\n",
    "\n",
    "The encoded name is known (perhaps incorrectly) as a personal numeric identifier (PNI). The encoded name can never contain more than 6 alpha only characters.\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Match_rating_approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Metaphone, MRA includes both hashing rules and their distance measure. It is suitable for small vocabularies and searching for abbreviations and acronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyphonetics import MatchingRatingApproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_mra(word):\n",
    "    mra= MatchingRatingApproach()\n",
    "    cand_list =[]\n",
    "    best_mra= 0\n",
    "    for current_string in vocabulary:\n",
    "        current_score = mra.distance(word, current_string)\n",
    "        if(current_score == best_mra):\n",
    "            best_mra = current_score\n",
    "            cand_list.append(current_string)\n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tresses',\n",
       " 'truss',\n",
       " 'tres',\n",
       " 'tutors',\n",
       " 'trousseau',\n",
       " 'tarsus',\n",
       " 'terse',\n",
       " 'terrors',\n",
       " 'trees',\n",
       " 'tiers',\n",
       " 'tories',\n",
       " 'tears',\n",
       " 'tries',\n",
       " 'taras',\n",
       " 'tires',\n",
       " 'teres']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_mra('treeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try more complex one with riiiiight\n",
    "candidates_mra('riiiiight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_mra_bestprobability(word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    try:\n",
    "        return max(candidates_mra (word), key=P)\n",
    "    except:\n",
    "        ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tears'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_mra_bestprobability('treeees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try more complex one with riiiight\n",
    "correction_mra_bestprobability('riiiiight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we can see that RefinedSoundex is outperfoming Metaphone and MRA on two examples. But we will come back to more robust testing later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word distance algortihms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance measures are methods that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other.\n",
    "\n",
    "In this part we will compare different types of edit distance models:\n",
    "- Levenshtein Distance\n",
    "- Damerau-Levenshtein Distance\n",
    "- Jaro-Winkler Distance\n",
    "- Hamming Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to play with different distance algorithms we will rely on python jellyfish library and impelement our customized functions to asess and choose the candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above edit distance, we will write a function that calculates all words with minimal edit distance to the misspelled word. \n",
    "The sub tasks involved are: \n",
    "- Collect the set of all unique words in vocabulary\n",
    "- Find the minimal edit distance, that is the lowest value for the function edit_distance between token and a word in train\n",
    "- Output all unique words in train that have this same (minimal) edit_distance value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jellyfish\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after the Soviet mathematician Vladimir Levenshtein, who considered this distance in 1965\n",
    "\n",
    "https://en.wikipedia.org/wiki/Levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows deletion, insertion and substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edit distance returns the number of changes to transform one word to another\n",
    "jellyfish.levenshtein_distance('simone', 'siomne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_lev (word):\n",
    "    #Creating List containing edit distances\n",
    "    edit_list=[]\n",
    "    \n",
    "    #Creating list containg possible candidates for right spelling\n",
    "    cand_list=[]\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        edit_list.append(jellyfish.levenshtein_distance(word,vocabulary[i]))\n",
    "        \n",
    "    #Getting the minimum value for edit distance\n",
    "    minimun=min(edit_list)\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        if(jellyfish.levenshtein_distance(word, vocabulary[i])==minimun):\n",
    "            cand_list.append(vocabulary[i])\n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stone', 'shone', 'omne', 'sloane']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_lev('siomne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_lev(word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    return max(candidates_lev(word), key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stone'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_lev('siomne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Damerau-Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Damerau–Levenshtein distance (named after Frederick J. Damerau and Vladimir I. Levenshtein is a string metric for measuring the edit distance between two sequences. Informally, the Damerau–Levenshtein distance between two words is the minimum number of operations (consisting of insertions, deletions or substitutions of a single character, or transposition of two adjacent characters) required to change one word into the other.\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As compared to Levenstein one it takes into account the transposition of two adjacent characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.damerau_levenshtein_distance('simone', 'siomne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_dam_lev (word):\n",
    "    #Creating List containing edit distances\n",
    "    edit_list=[]\n",
    "    \n",
    "    #Creating list containg possible candidates for right spelling\n",
    "    cand_list=[]\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        edit_list.append(jellyfish.damerau_levenshtein_distance(word,vocabulary[i]))\n",
    "        \n",
    "    #Getting the minimum value for edit distance\n",
    "    minimun=min(edit_list)\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        if(jellyfish.damerau_levenshtein_distance(word, vocabulary[i])==minimun):\n",
    "            cand_list.append(vocabulary[i])\n",
    "    \n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simons', 'simon']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_dam_lev('simone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_dam_lev(word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    return max(candidates_dam_lev(word), key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simon'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_dam_lev('simone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaro-Winkler Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures the minimum number of single-character transpositions required to change one word into the other, giving more favorable ratings to strings that match from the beginning\n",
    "\n",
    "https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.jaro_winkler('simone', 'siomne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6952380952380953"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.jaro_winkler('treeeeeeeeeees', 'trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.jaro_winkler('no', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_jaro_winkler(word):\n",
    "    #Creating List containing edit distances\n",
    "    edit_list=[]\n",
    "    \n",
    "    #Creating list containg possible candidates for right spelling\n",
    "    cand_list=[]\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        edit_list.append(jellyfish.jaro_winkler(word,vocabulary[i]))\n",
    "        \n",
    "    #Getting the maximum value for edit distance\n",
    "    maximum=max(edit_list)\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        if(jellyfish.jaro_winkler(word, vocabulary[i])==maximum):\n",
    "            cand_list.append(vocabulary[i])\n",
    "    \n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simon']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_jaro_winkler('siomne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_jaro_winkler(word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    return max(candidates_jaro_winkler(word), key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simon'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_jaro_winkler('siomne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hamming_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows only substitution, hence, it only applies to strings of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.hamming_distance('treek', 'trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_hamm (word):\n",
    "    #Creating List containing edit distances\n",
    "    edit_list=[]\n",
    "    \n",
    "    #Creating list containg possible candidates for right spelling\n",
    "    cand_list=[]\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        edit_list.append(jellyfish.hamming_distance(word,vocab[i]))\n",
    "        \n",
    "    #Getting the minimum value for edit distance\n",
    "    minimun=min(edit_list)\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        if(jellyfish.hamming_distance(word, vocabulary[i])==minimun):\n",
    "            cand_list.append(vocabulary[i])\n",
    "    \n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['creek', 'greek', 'tree', 'trees']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_hamm ('treek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_hamm (word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    return max(candidates_hamm (word), key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trees'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_hamm ('treek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leventsein and Damerau-Levenshtein weighted on keyboards distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we want to compare two strings, and and allow for typos, based on the physical distance between keys on Qwerty keyboard. \n",
    "\n",
    "In other words, the w algorithm should prefer \"yelephone\" to \"zelephone\" since the \"y\" key is located nearer to the \"t\" key than to the \"z\" key on most keyboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "keyboard_cartesian = {'q': {'x':0, 'y':0}, 'w': {'x':1, 'y':0}, 'e': {'x':2, 'y':0}, 'r': {'x':3, 'y':0}, 't': {'x':4, 'y':0}, 'y': {'x':5, 'y':0}, 'u': {'x':6, 'y':0}, 'i': {'x':7, 'y':0}, 'o': {'x':8, 'y':0}, 'p': {'x':9, 'y':0}, 'a': {'x':0, 'y':1},'z': {'x':0, 'y':2},'s': {'x':1, 'y':1},'x': {'x':1, 'y':2},'d': {'x':2, 'y':1},'c': {'x':2, 'y':2}, 'f': {'x':3, 'y':1}, 'b': {'x':4, 'y':2}, 'm': {'x':5, 'y':2}, 'j': {'x':6, 'y':1}, 'g': {'x':4, 'y':1}, 'h': {'x':5, 'y':1}, 'j': {'x':6, 'y':1}, 'k': {'x':7, 'y':1}, 'l': {'x':8, 'y':1}, 'v': {'x':3, 'y':2}, 'n': {'x':5, 'y':2}, }\n",
    "\n",
    "def keyboard_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x'] - keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y'] - keyboard_cartesian[b]['y'])**2\n",
    "    return math.sqrt(X+Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing closest one\n",
    "keyboard_distance ('q','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.219544457292887"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the faraway  one\n",
    "keyboard_distance('z','p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account this development, we created the own probability function, weighted by qwerty keyboard distance: \n",
    "\n",
    "https://metacpan.org/pod/release/KRBURTON/String-KeyboardDistance-1.01/KeyboardDistance.pm#qwerty_keyboard_distance_match\n",
    "\n",
    "\n",
    "Pr = 1 - ( D / (L * M) )\n",
    "Where D is the distance between the two strings, L is the length of the longer string, and M is the maximum character distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "\n",
    "def weighted_lev_distance (string1, string2):\n",
    "    \n",
    "    key_dist_lev = []\n",
    "    \n",
    "    max_len=max(len(string1), len(string2))\n",
    "\n",
    "      \n",
    "    def splitter1(string1):\n",
    "        charlist = list(string1) \n",
    "        letters1 = ' '.join(charlist).split()\n",
    "        return letters1\n",
    "\n",
    "    \n",
    "    def splitter2(string2):\n",
    "        charlist = list(string2) \n",
    "        letters2= ' '.join(charlist).split()\n",
    "        return letters2\n",
    "    \n",
    "    for a in splitter1(string1):\n",
    "        for b in splitter2(string2):\n",
    "            key_dist_lev.append(keyboard_distance(a,b))\n",
    "    max_key_dist_lev=(max(key_dist_lev))   \n",
    "     \n",
    "    proba= (1 - (jellyfish.levenshtein_distance(string1, string2)/(max_len*max_key_dist_lev)))\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_dau_lev_distance (string1, string2):\n",
    "    \n",
    "    key_dist_dau_lev = []\n",
    "    \n",
    "    max_len=max(len(string1), len(string2))\n",
    "\n",
    "      \n",
    "    def splitter1(string1):\n",
    "        charlist = list(string1) \n",
    "        letters1 = ' '.join(charlist).split()\n",
    "        return letters1\n",
    "\n",
    "    \n",
    "    def splitter2(string2):\n",
    "        charlist = list(string2) \n",
    "        letters2= ' '.join(charlist).split()\n",
    "        return letters2\n",
    "    \n",
    "    for a in splitter1(string1):\n",
    "        for b in splitter2(string2):\n",
    "            key_dist_dau_lev.append(keyboard_distance(a,b))\n",
    "    max_key_dist_dau_lev=(max(key_dist_dau_lev))   \n",
    "     \n",
    "    proba= (1 - (jellyfish.damerau_levenshtein_distance(string1, string2)/(max_len*max_key_dist_dau_lev)))\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_dau_lev_distance ('no', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7781199215099084"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_dau_lev_distance ('no', 'onoooooooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841269841269842"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_dau_lev_distance ('telephone', 'yelephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841269841269842"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_dau_lev_distance ('telephone', 'oelephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_dam_lev_keyboard (word):\n",
    "    #Creating List containing edit distances\n",
    "    probability_list=[]\n",
    "    \n",
    "    #Creating list containg possible candidates for right spelling\n",
    "    cand_list=[]\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        probability_list.append(weighted_dau_lev_distance(word,vocabulary[i]))\n",
    "        \n",
    "    #Getting the max probabilit for edit distance\n",
    "    maximum=max(probability_list)\n",
    "    \n",
    "    for i in range(0,len(vocabulary)):\n",
    "        if(weighted_dau_lev_distance(word, vocabulary[i])==maximum):\n",
    "            cand_list.append(vocabulary[i])\n",
    "    \n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['telephone']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_dam_lev_keyboard ('zzyelephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_dam_lev_keyboard(word): \n",
    "    #There is no specifications how to choose the candidate, we will use the most probable spelling correction for word\n",
    "    return max(candidates_dam_lev_keyboard(word), key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'telephone'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_dam_lev_keyboard ('zzyelephone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly comparing results with rest methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zzyelephone'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('zzyelephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'telephone'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_lev('zzyelephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'telephone'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_dam_lev('zzyelephone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, n-gram means splitting a string in sequences with the length n. So if we have this string “abcde”, then bigrams are: ab, bc, cd, and de while trigrams will be: abc, bcd, and cde while 4-grams will be abcd, and bcde.\n",
    "\n",
    "Since out testing words consist of 3 words+ we will test only the perfomance of unigrams use bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "string='treeeeees'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tresses'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correction_unigrams (entries=[string]):\n",
    "    corrected = []\n",
    "    dist_old = 1\n",
    "    best_match = None\n",
    "    for w in entries:\n",
    "        w_0=w[0]\n",
    "        for c in vocabulary:\n",
    "            dist_new = nltk.jaccard_distance(set(nltk.ngrams(c, n=1)),set(nltk.ngrams(w, n=1)))\n",
    "            if dist_new<dist_old and w_0==c[0]:\n",
    "                best_match = c\n",
    "                dist_old = dist_new \n",
    "        corrected.append(best_match)\n",
    "        return ', '.join(map(str, corrected))\n",
    "    \n",
    "correction_unigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trees, trees'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correction_bigrams (entries=[string]):\n",
    "    corrected = []\n",
    "    dist_old = 1\n",
    "    best_match = None\n",
    "    for w in entries:\n",
    "        w_0=w[0]\n",
    "        for c in vocabulary:\n",
    "            dist_new = nltk.jaccard_distance(set(nltk.bigrams(c)),set(nltk.bigrams(w)))\n",
    "            if dist_new < dist_old and w_0==c[0]:\n",
    "                best_match = c\n",
    "                dist_old = dist_new\n",
    "        corrected.append(best_match)\n",
    "        corrected.append(best_match)\n",
    "        return ', '.join(map(str, corrected))\n",
    "    \n",
    "correction_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison (evaluation of corrections) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 2)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspellings = pd.read_excel('C:/Users/serge/Desktop/Datasets/misspelings_short.xlsx')\n",
    "misp_list = list(misspellings['errors'])\n",
    "correct_answers = list(misspellings['correct'])\n",
    "\n",
    "misspellings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the initial dataframe\n",
    "\n",
    "#Misspellings = misspelt words\n",
    "#Correct_answers = answers of truth set\n",
    "\n",
    "data = pd.DataFrame({'misspellings':misp_list,'correct_answers':correct_answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspellings</th>\n",
       "      <th>correct_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abilty</td>\n",
       "      <td>ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abraod</td>\n",
       "      <td>abroad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acedemic</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accesion</td>\n",
       "      <td>accession</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accomodate</td>\n",
       "      <td>accommodate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspellings correct_answers\n",
       "0       abilty         ability\n",
       "1       abraod          abroad\n",
       "2     acedemic        academic\n",
       "3     accesion       accession\n",
       "4   accomodate     accommodate"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step adding the best candidates from all the techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.429903984069824 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Norvig (baseline corrector)\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Norviq_speed=(time.time() - start_time)\n",
    "\n",
    "data['Norviq']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 673.3390271663666 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Soundex\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_soundex_bestprobability(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Soundex_speed=(time.time() - start_time)\n",
    "\n",
    "data['Soundex']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1868.1607694625854 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Metaphone\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_metaphone_bestprobability(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Metaphone_speed=(time.time() - start_time)\n",
    "\n",
    "data['Metaphone']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 682.3088150024414 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#MRA\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_mra_bestprobability(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "MRA_speed=(time.time() - start_time)\n",
    "\n",
    "data['MRA']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 17.26406455039978 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Leventstein\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_lev(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Lev_speed=(time.time() - start_time)\n",
    "\n",
    "data['Levenstein']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 29.3111891746521 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Damerau Levenstein\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_dam_lev(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Damerau_Lev_speed=(time.time() - start_time)\n",
    "\n",
    "data['Damerau_Levenstein']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 52.08599090576172 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Jaro Winkler\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_jaro_winkler(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Jaro_Winkler_speed=(time.time() - start_time)\n",
    "\n",
    "data['Jaro_Winkler']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.240901470184326 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Hamming\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_hamm(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Hamming_speed=(time.time() - start_time)\n",
    "\n",
    "data['Hamming']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 128.05072379112244 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Unigrams\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_unigrams(entries=[i]))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Unigrams_speed=(time.time() - start_time)\n",
    "\n",
    "data['Unigrams']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 152.0656476020813 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Bigrams\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_bigrams(entries=[i]))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Bigrams_speed=(time.time() - start_time)\n",
    "\n",
    "data['Bigrams']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3973.498164653778 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Damerau Levenstein Distance weighted by physical Keyboard distance (QWERTY)\n",
    "start_time = time.time()\n",
    "\n",
    "df = []\n",
    "for i in misp_list:\n",
    "    df.append(correction_dam_lev_keyboard(i))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Dam_Lev_Keyboardweighted_speed=(time.time() - start_time)\n",
    "\n",
    "data['Dam_Lev_Keyboardweighted']=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspellings</th>\n",
       "      <th>correct_answers</th>\n",
       "      <th>Norviq</th>\n",
       "      <th>Soundex</th>\n",
       "      <th>Metaphone</th>\n",
       "      <th>MRA</th>\n",
       "      <th>Levenstein</th>\n",
       "      <th>Damerau_Levenstein</th>\n",
       "      <th>Jaro_Winkler</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>Unigrams</th>\n",
       "      <th>Bigrams</th>\n",
       "      <th>Dam_Lev_Keyboardweighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abilty</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>None</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>guilty</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability, ability</td>\n",
       "      <td>ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abraod</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>afraid</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>afraid</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abraham, abraham</td>\n",
       "      <td>abroad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acedemic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>None</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>acute</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic, academic</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accesion</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>occasion</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession, accession</td>\n",
       "      <td>accession</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accomodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accumulated</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate, accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspellings correct_answers       Norviq      Soundex    Metaphone  \\\n",
       "0       abilty         ability      ability         None      ability   \n",
       "1       abraod          abroad       abroad       abroad       abroad   \n",
       "2     acedemic        academic     academic     academic         None   \n",
       "3     accesion       accession    accession    accession    accession   \n",
       "4   accomodate     accommodate  accommodate  accommodate  accommodate   \n",
       "\n",
       "           MRA   Levenstein Damerau_Levenstein Jaro_Winkler      Hamming  \\\n",
       "0      ability      ability            ability      ability       guilty   \n",
       "1       abroad       afraid             abroad       abroad       afraid   \n",
       "2     academic     academic           academic     academic        acute   \n",
       "3    accession    accession          accession    accession     occasion   \n",
       "4  accommodate  accommodate        accommodate  accommodate  accumulated   \n",
       "\n",
       "      Unigrams                   Bigrams Dam_Lev_Keyboardweighted  \n",
       "0      ability          ability, ability                  ability  \n",
       "1       abroad          abraham, abraham                   abroad  \n",
       "2     academic        academic, academic                 academic  \n",
       "3    accession      accession, accession                accession  \n",
       "4  accommodate  accommodate, accommodate              accommodate  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating binary variable as to see where our function  (CORRECT_TEXT matches the truth set = CORRECT_ANSWERS)\n",
    "data['Norviq_accuracy']=data.apply(lambda row: row['Norviq'] == row['correct_answers'],axis=1)\n",
    "data['Soundex_accuracy']=data.apply(lambda row: row['Soundex'] == row['correct_answers'],axis=1)\n",
    "data['Metaphone_accuracy']=data.apply(lambda row: row['Metaphone'] == row['correct_answers'],axis=1)\n",
    "data['MRA_accuracy']=data.apply(lambda row: row['MRA'] == row['correct_answers'],axis=1)\n",
    "data['Levenstein_accuracy']=data.apply(lambda row: row['Levenstein'] == row['correct_answers'],axis=1)\n",
    "data['Damerau_Levenstein_accuracy']=data.apply(lambda row: row['Damerau_Levenstein'] == row['correct_answers'],axis=1)\n",
    "data['Jaro_Winkler_accuracy']=data.apply(lambda row: row['Jaro_Winkler'] == row['correct_answers'],axis=1)\n",
    "data['Hamming_accuracy']=data.apply(lambda row: row['Hamming'] == row['correct_answers'],axis=1)\n",
    "data['Unigrams_accuracy']=data.apply(lambda row: row['Unigrams'] == row['correct_answers'],axis=1)\n",
    "data['Bigrams_accuracy']=data.apply(lambda row: row['Bigrams'] == row['correct_answers'],axis=1)\n",
    "data['Dam_Lev_Keyboardweighted_accuracy']=data.apply(lambda row: row['Dam_Lev_Keyboardweighted'] == row['correct_answers'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['misspellings', 'correct_answers', 'Norviq', 'Soundex', 'Metaphone',\n",
       "       'MRA', 'Levenstein', 'Damerau_Levenstein', 'Jaro_Winkler', 'Hamming',\n",
       "       'Unigrams', 'Bigrams', 'Dam_Lev_Keyboardweighted', 'Norviq_accuracy',\n",
       "       'Soundex_accuracy', 'Metaphone_accuracy', 'MRA_accuracy',\n",
       "       'Levenstein_accuracy', 'Damerau_Levenstein_accuracy',\n",
       "       'Jaro_Winkler_accuracy', 'Hamming_accuracy', 'Unigrams_accuracy',\n",
       "       'Bigrams_accuracy', 'Dam_Lev_Keyboardweighted_accuracy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_binary=['Norviq_accuracy', 'Soundex_accuracy', 'Metaphone_accuracy', 'MRA_accuracy', 'Levenstein_accuracy',\n",
    "       'Damerau_Levenstein_accuracy', 'Jaro_Winkler_accuracy',\n",
    "       'Hamming_accuracy',  'Unigrams_accuracy', 'Bigrams_accuracy', 'Dam_Lev_Keyboardweighted_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This transforms the correct column previously created from TRUE & FALSE to 1 and zero\n",
    "data[col_binary]=data[col_binary]*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspellings</th>\n",
       "      <th>correct_answers</th>\n",
       "      <th>Norviq</th>\n",
       "      <th>Soundex</th>\n",
       "      <th>Metaphone</th>\n",
       "      <th>MRA</th>\n",
       "      <th>Levenstein</th>\n",
       "      <th>Damerau_Levenstein</th>\n",
       "      <th>Jaro_Winkler</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>...</th>\n",
       "      <th>Soundex_accuracy</th>\n",
       "      <th>Metaphone_accuracy</th>\n",
       "      <th>MRA_accuracy</th>\n",
       "      <th>Levenstein_accuracy</th>\n",
       "      <th>Damerau_Levenstein_accuracy</th>\n",
       "      <th>Jaro_Winkler_accuracy</th>\n",
       "      <th>Hamming_accuracy</th>\n",
       "      <th>Unigrams_accuracy</th>\n",
       "      <th>Bigrams_accuracy</th>\n",
       "      <th>Dam_Lev_Keyboardweighted_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abilty</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>None</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>ability</td>\n",
       "      <td>guilty</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abraod</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>afraid</td>\n",
       "      <td>abroad</td>\n",
       "      <td>abroad</td>\n",
       "      <td>afraid</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acedemic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>None</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>acute</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accesion</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>accession</td>\n",
       "      <td>occasion</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accomodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>accumulated</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspellings correct_answers       Norviq      Soundex    Metaphone  \\\n",
       "0       abilty         ability      ability         None      ability   \n",
       "1       abraod          abroad       abroad       abroad       abroad   \n",
       "2     acedemic        academic     academic     academic         None   \n",
       "3     accesion       accession    accession    accession    accession   \n",
       "4   accomodate     accommodate  accommodate  accommodate  accommodate   \n",
       "\n",
       "           MRA   Levenstein Damerau_Levenstein Jaro_Winkler      Hamming  ...  \\\n",
       "0      ability      ability            ability      ability       guilty  ...   \n",
       "1       abroad       afraid             abroad       abroad       afraid  ...   \n",
       "2     academic     academic           academic     academic        acute  ...   \n",
       "3    accession    accession          accession    accession     occasion  ...   \n",
       "4  accommodate  accommodate        accommodate  accommodate  accumulated  ...   \n",
       "\n",
       "  Soundex_accuracy Metaphone_accuracy MRA_accuracy  Levenstein_accuracy  \\\n",
       "0                0                  1            1                    1   \n",
       "1                1                  1            1                    0   \n",
       "2                1                  0            1                    1   \n",
       "3                1                  1            1                    1   \n",
       "4                1                  1            1                    1   \n",
       "\n",
       "   Damerau_Levenstein_accuracy  Jaro_Winkler_accuracy  Hamming_accuracy  \\\n",
       "0                            1                      1                 0   \n",
       "1                            1                      1                 0   \n",
       "2                            1                      1                 0   \n",
       "3                            1                      1                 0   \n",
       "4                            1                      1                 0   \n",
       "\n",
       "   Unigrams_accuracy  Bigrams_accuracy  Dam_Lev_Keyboardweighted_accuracy  \n",
       "0                  1                 0                                  1  \n",
       "1                  1                 0                                  1  \n",
       "2                  1                 0                                  1  \n",
       "3                  1                 0                                  1  \n",
       "4                  1                 0                                  1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.736979\n",
       "0    0.263021\n",
       "Name: Norviq_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.679688\n",
       "1    0.320312\n",
       "Name: Soundex_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.601562\n",
       "1    0.398438\n",
       "Name: Metaphone_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.523438\n",
       "1    0.476562\n",
       "Name: MRA_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0.669271\n",
       "0    0.330729\n",
       "Name: Levenstein_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0.739583\n",
       "0    0.260417\n",
       "Name: Damerau_Levenstein_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0.700521\n",
       "0    0.299479\n",
       "Name: Jaro_Winkler_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.802083\n",
       "1    0.197917\n",
       "Name: Hamming_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0.513021\n",
       "0    0.486979\n",
       "Name: Unigrams_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: Bigrams_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0.721354\n",
       "0    0.278646\n",
       "Name: Dam_Lev_Keyboardweighted_accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in data[col_binary]:\n",
    "    display(data[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_measurements= [Norviq_speed, Soundex_speed, Metaphone_speed, MRA_speed, Lev_speed, Damerau_Lev_speed,  Jaro_Winkler_speed, Hamming_speed, Unigrams_speed, Bigrams_speed, Dam_Lev_Keyboardweighted_speed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "673.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1868.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "682.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "52.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "128.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "152.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3973.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for algo in speed_measurements:\n",
    "    display(np.round(algo,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time speed (in seconds)</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Damerau-Levenstein</th>\n",
       "      <td>29.3</td>\n",
       "      <td>73.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norviq</th>\n",
       "      <td>8.4</td>\n",
       "      <td>73.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dam-Lev weighted by Keyboard Distance</th>\n",
       "      <td>3973.5</td>\n",
       "      <td>72.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <td>52.1</td>\n",
       "      <td>70.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Levenstein</th>\n",
       "      <td>17.3</td>\n",
       "      <td>66.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigrams</th>\n",
       "      <td>128.1</td>\n",
       "      <td>51.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRA</th>\n",
       "      <td>682.3</td>\n",
       "      <td>47.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metaphone</th>\n",
       "      <td>1868.2</td>\n",
       "      <td>39.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soundex</th>\n",
       "      <td>673.3</td>\n",
       "      <td>32.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming</th>\n",
       "      <td>11.2</td>\n",
       "      <td>19.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigrams</th>\n",
       "      <td>152.1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Time speed (in seconds)  Accuracy\n",
       "Damerau-Levenstein                                        29.3     73.96\n",
       "Norviq                                                     8.4     73.70\n",
       "Dam-Lev weighted by Keyboard Distance                   3973.5     72.14\n",
       "Jaro-Winkler                                              52.1     70.05\n",
       "Levenstein                                                17.3     66.93\n",
       "Unigrams                                                 128.1     51.30\n",
       "MRA                                                      682.3     47.66\n",
       "Metaphone                                               1868.2     39.84\n",
       "Soundex                                                  673.3     32.03\n",
       "Hamming                                                   11.2     19.79\n",
       "Bigrams                                                  152.1      0.00"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics=pd.DataFrame(columns={'Accuracy', 'Time speed (in seconds)'}, index=['Norviq', 'Soundex', 'Metaphone', 'MRA', 'Levenstein', 'Damerau-Levenstein', 'Jaro-Winkler', 'Hamming', 'Unigrams', 'Bigrams','Dam-Lev weighted by Keyboard Distance'])\n",
    "\n",
    "#Calculating the accuracy\n",
    "accuracy=[]\n",
    "for col in data[col_binary]:\n",
    "    accuracy.append(round(sum(data[col]==1)*100/sum(data[col].value_counts()),2))\n",
    "\n",
    "statistics['Accuracy']=accuracy\n",
    "\n",
    "\n",
    "#Calculating the speed\n",
    "speed=[]\n",
    "for algo in speed_measurements:\n",
    "    speed.append(np.round(algo,1))\n",
    "    \n",
    "statistics['Time speed (in seconds)']=speed\n",
    "\n",
    "#Showing the results\n",
    "statistics.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter one word: sdsd\n",
      "Spell-checked version by Group C: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'said'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interface():\n",
    "    a = input(\"Enter one word: \")\n",
    "    b = a \n",
    "    print (\"Spell-checked version by Group C: \")\n",
    "    return correction_dam_lev(b)\n",
    "\n",
    "interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
